{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCbpncsEOyomc/7jbpzWvm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8M-xDzd48DHs"},"outputs":[],"source":[]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.utils import get_file\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import os\n","# Load your dataset\n","# For example, you can use the Gutenberg corpus from NLTK\n","import nltk\n","nltk.download('gutenberg')\n","from nltk.corpus import gutenberg\n","text = gutenberg.raw('shakespeare-hamlet.txt')\n","\n","\n","# Preprocessing the text\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Creating input sequences and corresponding outputs\n","input_sequences = []\n","for line in text.split('\\n'):\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Padding sequences\n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# Creating predictors and label\n","X, y = input_sequences[:,:-1],input_sequences[:,-1]\n","y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n","\n","# Building the model\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n","model.add(LSTM(150, return_sequences=True))\n","model.add(LSTM(100))\n","model.add(Dense(total_words, activation='softmax'))\n","\n","# Compiling the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()\n","\n","# Training the model\n","model.fit(X, y, epochs=50, verbose=1)\n","def generate_story(seed_text, sentences, model, max_sequence_len):\n","    generated_story = seed_text\n","    for _ in range(sentences):\n","        for _ in range(10):  # Generate 10 words per sentence\n","            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","            token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","            predicted_probs = model.predict(token_list, verbose=0)[0]\n","            predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n","            output_word = tokenizer.index_word[predicted_index] if predicted_index > 0 else ''\n","            seed_text += \" \" + output_word\n","            generated_story += \" \" + output_word\n","            if output_word == '.':\n","                break  # End the sentence if period (.) is generated\n","        seed_text = '.'  # Start a new sentence\n","        generated_story += \"\\n\"  # Start a new line for each new sentence\n","    return generated_story\n","\n","# Generate a story\n","Text=input(\"Enter the text to generate Passages: \")\n","generated_story = generate_story(Text, 5, model, max_sequence_len)\n","print(generated_story)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c1392b28-bc02-4ffb-b1bd-38fdb2e634e8","id":"nttF3hTZ0udz","executionInfo":{"status":"ok","timestamp":1712324189648,"user_tz":420,"elapsed":2410717,"user":{"displayName":"nandha nandha","userId":"05018310402603508469"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 13, 100)           481800    \n","                                                                 \n"," lstm_4 (LSTM)               (None, 13, 150)           150600    \n","                                                                 \n"," lstm_5 (LSTM)               (None, 100)               100400    \n","                                                                 \n"," dense_2 (Dense)             (None, 4818)              486618    \n","                                                                 \n","=================================================================\n","Total params: 1219418 (4.65 MB)\n","Trainable params: 1219418 (4.65 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/50\n","805/805 [==============================] - 50s 57ms/step - loss: 6.8889 - accuracy: 0.0333\n","Epoch 2/50\n","805/805 [==============================] - 47s 58ms/step - loss: 6.4705 - accuracy: 0.0408\n","Epoch 3/50\n","805/805 [==============================] - 48s 60ms/step - loss: 6.2917 - accuracy: 0.0499\n","Epoch 4/50\n","805/805 [==============================] - 47s 59ms/step - loss: 6.1280 - accuracy: 0.0550\n","Epoch 5/50\n","805/805 [==============================] - 48s 59ms/step - loss: 5.9719 - accuracy: 0.0644\n","Epoch 6/50\n","805/805 [==============================] - 47s 59ms/step - loss: 5.8098 - accuracy: 0.0754\n","Epoch 7/50\n","805/805 [==============================] - 46s 57ms/step - loss: 5.6532 - accuracy: 0.0850\n","Epoch 8/50\n","805/805 [==============================] - 47s 58ms/step - loss: 5.5025 - accuracy: 0.0924\n","Epoch 9/50\n","805/805 [==============================] - 47s 58ms/step - loss: 5.3607 - accuracy: 0.0991\n","Epoch 10/50\n","805/805 [==============================] - 47s 58ms/step - loss: 5.2234 - accuracy: 0.1057\n","Epoch 11/50\n","805/805 [==============================] - 46s 57ms/step - loss: 5.0858 - accuracy: 0.1119\n","Epoch 12/50\n","805/805 [==============================] - 47s 59ms/step - loss: 4.9466 - accuracy: 0.1167\n","Epoch 13/50\n","805/805 [==============================] - 48s 59ms/step - loss: 4.8105 - accuracy: 0.1225\n","Epoch 14/50\n","805/805 [==============================] - 48s 60ms/step - loss: 4.6805 - accuracy: 0.1291\n","Epoch 15/50\n","805/805 [==============================] - 50s 62ms/step - loss: 4.5516 - accuracy: 0.1342\n","Epoch 16/50\n","805/805 [==============================] - 51s 63ms/step - loss: 4.4275 - accuracy: 0.1417\n","Epoch 17/50\n","805/805 [==============================] - 47s 58ms/step - loss: 4.3044 - accuracy: 0.1498\n","Epoch 18/50\n","805/805 [==============================] - 48s 59ms/step - loss: 4.1880 - accuracy: 0.1596\n","Epoch 19/50\n","805/805 [==============================] - 47s 58ms/step - loss: 4.0756 - accuracy: 0.1730\n","Epoch 20/50\n","805/805 [==============================] - 47s 58ms/step - loss: 3.9680 - accuracy: 0.1885\n","Epoch 21/50\n","805/805 [==============================] - 46s 57ms/step - loss: 3.8686 - accuracy: 0.2055\n","Epoch 22/50\n","805/805 [==============================] - 47s 59ms/step - loss: 3.7696 - accuracy: 0.2214\n","Epoch 23/50\n","805/805 [==============================] - 47s 59ms/step - loss: 3.6782 - accuracy: 0.2396\n","Epoch 24/50\n","805/805 [==============================] - 45s 56ms/step - loss: 3.5919 - accuracy: 0.2549\n","Epoch 25/50\n","805/805 [==============================] - 47s 58ms/step - loss: 3.5113 - accuracy: 0.2696\n","Epoch 26/50\n","805/805 [==============================] - 47s 59ms/step - loss: 3.4327 - accuracy: 0.2828\n","Epoch 27/50\n","805/805 [==============================] - 47s 59ms/step - loss: 3.3572 - accuracy: 0.2962\n","Epoch 28/50\n","805/805 [==============================] - 46s 58ms/step - loss: 3.2862 - accuracy: 0.3112\n","Epoch 29/50\n","805/805 [==============================] - 48s 59ms/step - loss: 3.2184 - accuracy: 0.3218\n","Epoch 30/50\n","805/805 [==============================] - 48s 59ms/step - loss: 3.1528 - accuracy: 0.3331\n","Epoch 31/50\n","805/805 [==============================] - 47s 59ms/step - loss: 3.0913 - accuracy: 0.3440\n","Epoch 32/50\n","805/805 [==============================] - 48s 60ms/step - loss: 3.0290 - accuracy: 0.3573\n","Epoch 33/50\n","805/805 [==============================] - 46s 58ms/step - loss: 2.9716 - accuracy: 0.3669\n","Epoch 34/50\n","805/805 [==============================] - 47s 59ms/step - loss: 2.9145 - accuracy: 0.3774\n","Epoch 35/50\n","805/805 [==============================] - 48s 59ms/step - loss: 2.8581 - accuracy: 0.3873\n","Epoch 36/50\n","805/805 [==============================] - 47s 59ms/step - loss: 2.8048 - accuracy: 0.3976\n","Epoch 37/50\n","805/805 [==============================] - 46s 58ms/step - loss: 2.7558 - accuracy: 0.4079\n","Epoch 38/50\n","805/805 [==============================] - 48s 59ms/step - loss: 2.7012 - accuracy: 0.4199\n","Epoch 39/50\n","805/805 [==============================] - 48s 60ms/step - loss: 2.6529 - accuracy: 0.4273\n","Epoch 40/50\n","805/805 [==============================] - 48s 59ms/step - loss: 2.6044 - accuracy: 0.4355\n","Epoch 41/50\n","805/805 [==============================] - 48s 59ms/step - loss: 2.5571 - accuracy: 0.4471\n","Epoch 42/50\n","805/805 [==============================] - 46s 58ms/step - loss: 2.5117 - accuracy: 0.4527\n","Epoch 43/50\n","805/805 [==============================] - 48s 59ms/step - loss: 2.4654 - accuracy: 0.4630\n","Epoch 44/50\n","805/805 [==============================] - 47s 59ms/step - loss: 2.4237 - accuracy: 0.4733\n","Epoch 45/50\n","805/805 [==============================] - 47s 58ms/step - loss: 2.3781 - accuracy: 0.4826\n","Epoch 46/50\n","805/805 [==============================] - 47s 58ms/step - loss: 2.3365 - accuracy: 0.4895\n","Epoch 47/50\n","805/805 [==============================] - 47s 58ms/step - loss: 2.2947 - accuracy: 0.4972\n","Epoch 48/50\n","805/805 [==============================] - 48s 59ms/step - loss: 2.2538 - accuracy: 0.5058\n","Epoch 49/50\n","805/805 [==============================] - 47s 58ms/step - loss: 2.2148 - accuracy: 0.5159\n","Epoch 50/50\n","805/805 [==============================] - 46s 57ms/step - loss: 2.1774 - accuracy: 0.5214\n","Enter the text to generate Passages: wonders are wondering about your beauty\n","wonders are wondering about your beauty with the heauens giue quiet question in the begger is\n"," hamlet was a things confession to your owne man eare\n"," some poore weeds distemper'd with horrid way daughter ha's a\n"," this vnpreuayling he and worse then i fat if we\n"," let thy betters come in your faith be quiet scull\n","\n"]}]}]}